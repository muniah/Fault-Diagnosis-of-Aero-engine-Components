{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cee028c0",
   "metadata": {},
   "source": [
    "### Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1510544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Set a fixed seed value for reproducibility\n",
    "SEED = 1\n",
    "random.seed(SEED)            # Python random module\n",
    "np.random.seed(SEED)         # NumPy\n",
    "tf.random.set_seed(SEED)     # TensorFlow\n",
    "\n",
    "# Enforce deterministic behavior for GPU operations\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'  # Ensure deterministic execution\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'  # Deterministic cuDNN algorithms\n",
    "\n",
    "# Control GPU memory allocation (prevents TensorFlow from using all GPU memory)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)  # Enable memory growth\n",
    "\n",
    "# Restrict parallelism (ensures consistent execution order)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "\n",
    "import scipy.io \n",
    "from sklearn.metrics import (confusion_matrix, accuracy_score, classification_report, \n",
    "f1_score, precision_score, recall_score, roc_auc_score)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from scipy import signal\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras import layers, models, Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Conv1D, Flatten, MaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load 2HP CWRU bearing data from .mat files\n",
    "def import_data():\n",
    "    \"\"\"\n",
    "    Efficiently load CWRU bearing data from .mat files in the specified folder.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of NumPy arrays, each corresponding to a different bearing condition.\n",
    "    \"\"\"\n",
    "    folder_path_1 = os.path.join(os.getcwd(), 'CWRU_data', '2HP')\n",
    "\n",
    "    files = {\n",
    "        '99.mat': 'X099_DE_time',\n",
    "        '111.mat': 'X111_DE_time',\n",
    "        '124.mat': 'X124_DE_time',\n",
    "        '137.mat': 'X137_DE_time',\n",
    "        '176.mat': 'X176_DE_time',\n",
    "        '191.mat': 'X191_DE_time',\n",
    "        '203.mat': 'X203_DE_time',\n",
    "        '215.mat': 'X215_DE_time',\n",
    "        '228.mat': 'X228_DE_time',\n",
    "        '240.mat': 'X240_DE_time',\n",
    "    }\n",
    "\n",
    "    data_2HP = []\n",
    "    for file_name, key in files.items():\n",
    "        file_path = os.path.join(folder_path_1, file_name)\n",
    "        data_2HP.append(scipy.io.loadmat(file_path)[key])\n",
    "\n",
    "    folder_path_2 =  os.path.join(os.getcwd(), 'CWRU_data', '3HP')\n",
    "    files = {\n",
    "        '100.mat': 'X100_DE_time',\n",
    "        '112.mat': 'X112_DE_time',\n",
    "        '125.mat': 'X125_DE_time',\n",
    "        '138.mat': 'X138_DE_time',\n",
    "        '177.mat': 'X177_DE_time',\n",
    "        '192.mat': 'X192_DE_time',\n",
    "        '204.mat': 'X204_DE_time',\n",
    "        '217.mat': 'X217_DE_time',\n",
    "        '229.mat': 'X229_DE_time',\n",
    "        '241.mat': 'X241_DE_time',\n",
    "    }\n",
    "\n",
    "    data_3HP = []\n",
    "    for file_name, key in files.items():\n",
    "        file_path = os.path.join(folder_path_2, file_name)\n",
    "        data_3HP.append(scipy.io.loadmat(file_path)[key])\n",
    "\n",
    "    return data_2HP, data_3HP\n",
    "\n",
    "data_2HP, data_3HP = import_data()\n",
    "len(data_2HP), len(data_3HP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd605c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(data, interval_length, samples_per_block, ignore_points=0):\n",
    "    \"\"\"\n",
    "    Split the data into blocks of specified length, ignoring the first and last 'ignore_points'.\n",
    "    Args:\n",
    "        data (np.ndarray): The input data array to be split.\n",
    "        interval_length (int): The length of each interval for splitting.\n",
    "        samples_per_block (int): The number of samples in each block.\n",
    "        ignore_points (int): The number of points to ignore at the beginning and end of the data.\n",
    "    Returns:\n",
    "        np.ndarray: A 2D array where each row is a block of data.\n",
    "    \"\"\" \n",
    "    # Adjust data length to ignore the first and last 'ignore_points'\n",
    "    adjusted_length = len(data) - 2 * ignore_points\n",
    "\n",
    "    # Adjust the number of blocks\n",
    "    no_of_blocks = (round(adjusted_length / interval_length) - round(samples_per_block / interval_length) - 1)\n",
    "    split_data = np.zeros([no_of_blocks, samples_per_block])\n",
    "\n",
    "    for i in range(no_of_blocks):\n",
    "        # Skip the first 'ignore_points' and start sampling from that position\n",
    "        start_idx = ignore_points + i * interval_length\n",
    "        split_data[i, :] = data[start_idx:(start_idx + samples_per_block)].T\n",
    "\n",
    "        # print(\"Split data shape\", split_data[i, :].shape)\n",
    "\n",
    "    return split_data\n",
    "\n",
    "\n",
    "def data_preparation(data, interval_length, samples_per_block):\n",
    "  for count,i in enumerate(data):\n",
    "    split_data = sampling(i, interval_length, samples_per_block)\n",
    "    y = np.zeros([len(split_data),10])\n",
    "    y[:,count] = 1\n",
    "    y1 = np.zeros([len(split_data),1])\n",
    "    y1[:,0] = count\n",
    "    # Stack up and label the data   \n",
    "    if count==0:\n",
    "      X = split_data\n",
    "      y_positional = y\n",
    "      y_labels = y1\n",
    "    else:\n",
    "      X = np.append(X, split_data, axis=0)\n",
    "      y_positional = np.append(y_positional,y,axis=0)\n",
    "      y_labels = np.append(y_labels,y1,axis=0)\n",
    "  return X, y_positional, y_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611f7e70",
   "metadata": {},
   "source": [
    "### Overlapping Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73f4ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_length = 320 \n",
    "samples_per_block = 1600\n",
    "\n",
    "'''Y_CNN is of shape (n, 10) representing 10 classes as 10 columns. In each sample, for the class to which it belongs, \n",
    "the corresponding column value is marked 1 and the rest as 0, facilitating Softmax implementation in CNN \n",
    "Y is of shape (m, 1) where column values are between 0 and 9 representing the classes directly, 1-hot encoding'''\n",
    "\n",
    "X_2HP, y_onehot_2HP, y_2HP = data_preparation(data_2HP, interval_length, samples_per_block) \n",
    "X_3HP, y_onehot_3HP, y_3HP = data_preparation(data_3HP, interval_length, samples_per_block)\n",
    "\n",
    "print('Shape of input data (2HP) =', X_2HP.shape)\n",
    "print('Shape of one hot encoded label(2HP)  =', y_onehot_2HP.shape)\n",
    "print('Shape of labels (2HP) =', y_2HP.shape)\n",
    "\n",
    "print('Shape of input data (3HP) =', X_3HP.shape)\n",
    "print('Shape of one hot encoded label(3HP)  =', y_onehot_3HP.shape)\n",
    "print('Shape of labels (3HP) =', y_3HP.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038dd8ca",
   "metadata": {},
   "source": [
    "### Time-Based Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10537391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_stratified_split(X, y, train_ratio = 0.8):\n",
    "    num_classes = y.shape[1]\n",
    "    X_train, y_train, X_test, y_test = [], [], [], []\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        cls_indices = np.where(np.argmax(y, axis=1) == cls)[0]\n",
    "        n_train = int(train_ratio * len(cls_indices))\n",
    "        train_idx, test_idx = cls_indices[:n_train], cls_indices[n_train:]\n",
    "        X_train.append(X[train_idx])\n",
    "        # print(\"X_train shape:\", len(X_train))\n",
    "        y_train.append(y[train_idx])\n",
    "        # print(\"y_train shape:\", len(X_train))\n",
    "        X_test.append(X[test_idx])\n",
    "        # print(\"X_train shape:\", len(X_train))\n",
    "        y_test.append(y[test_idx])\n",
    "        # print(\"y_test shape:\", len(X_train))\n",
    "        print(f\"Class {cls}: Train size = {len(train_idx)}, Test size = {len(test_idx)}\")\n",
    "\n",
    "    return (\n",
    "        np.concatenate(X_train),\n",
    "        np.concatenate(y_train),\n",
    "        np.concatenate(X_test),\n",
    "        np.concatenate(y_test)\n",
    "    )\n",
    "\n",
    "X_train_2HP, y_train_2HP, X_test_2HP, y_test_2HP = time_series_stratified_split(X_2HP, y_onehot_2HP)\n",
    "X_train_3HP, y_train_3HP, X_test_3HP, y_test_3HP = time_series_stratified_split(X_3HP, y_onehot_3HP)\n",
    "\n",
    "print(\"Shape of training data (2HP):\", X_train_2HP.shape)\n",
    "print(\"Shape of training labels (2HP):\", y_train_2HP.shape)\n",
    "print(\"Shape of testing data (2HP):\", X_test_2HP.shape)\n",
    "print(\"Shape of testing labels (2HP):\", y_test_2HP.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f684d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold cross validation \n",
    "k_splits = 5\n",
    "kfold = StratifiedKFold(n_splits=k_splits, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a929bd64",
   "metadata": {},
   "source": [
    "### Baseline Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bb71ed",
   "metadata": {},
   "source": [
    "#### 2D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44944ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_2D():\n",
    "  def __init__(self):\n",
    "    self.model = self.CreateModel()\n",
    "\n",
    "  def CreateModel(self):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(filters=16, kernel_size=(3,3), padding='same',activation='relu', input_shape=(40,40,1)),\n",
    "        layers.MaxPool2D(pool_size=(2,2), padding='same'),\n",
    "\n",
    "        layers.Conv2D(filters=32, kernel_size=(3,3), padding ='same',activation='relu'),\n",
    "        layers.MaxPool2D(pool_size=(2,2), padding='same'),\n",
    "\n",
    "        layers.Conv2D(filters=64, kernel_size=(3,3),padding ='same', activation='relu'),\n",
    "        layers.MaxPool2D(pool_size=(2,2), padding='same'),\n",
    "\n",
    "        layers.Conv2D(filters=128, kernel_size=(3,3),padding ='same', activation='relu'),\n",
    "        layers.MaxPool2D(pool_size=(2,2), padding='same'),\n",
    "        layers.Flatten(),\n",
    "        \n",
    "        layers.Dense(100,activation='relu'),\n",
    "        layers.Dense(50,activation='relu'),\n",
    "        layers.Dense(10),\n",
    "        layers.Softmax()\n",
    "        ])\n",
    "    model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a070eda",
   "metadata": {},
   "source": [
    "#### 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d3d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_1D():\n",
    "    def __init__(self):\n",
    "        self.model = self.CreateModel()\n",
    "        self.model.summary()\n",
    "\n",
    "    def CreateModel(self):\n",
    "        model = models.Sequential([\n",
    "            layers.Conv1D(filters=16, kernel_size=3, strides=1, padding='same', activation='relu', input_shape=input_shape),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPool1D(pool_size=2),\n",
    "            \n",
    "            layers.Conv1D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPool1D(pool_size=2),\n",
    "\n",
    "            layers.Conv1D(filters=64, kernel_size=3, strides=1, padding='same', activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPool1D(pool_size=2),\n",
    "            layers.GlobalAveragePooling1D(),\n",
    "            \n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        # Optimizer with a slightly higher learning rate\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                      metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8e5199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define folder path and reshape the data for the models\n",
    "# File path name to save best models\n",
    "foldername = \"CNN2D_results_for_report/Baseline/\"\n",
    "os.makedirs(foldername, exist_ok=True)\n",
    "\n",
    "# Reshape the data for 2D CNN input\n",
    "X_train_2HP_2D = X_train_2HP.reshape(X_train_2HP.shape[0], 40, 40, 1)\n",
    "X_train_3HP_2D = X_train_3HP.reshape(X_train_3HP.shape[0], 40, 40, 1)\n",
    "y_train_2HP_2D = y_train_2HP.reshape(y_train_2HP.shape[0], 10)\n",
    "y_train_3HP_2D = y_train_3HP.reshape(y_train_3HP.shape[0], 10)\n",
    "\n",
    "\n",
    "X_test_2HP_2D = X_test_2HP.reshape(X_test_2HP.shape[0], 40, 40, 1)\n",
    "X_test_3HP_2D = X_test_3HP.reshape(X_test_3HP.shape[0], 40, 40, 1)  \n",
    "y_test_2HP_2D = y_test_2HP.reshape(y_test_2HP.shape[0], 10)\n",
    "y_test_3HP_2D = y_test_3HP.reshape(y_test_3HP.shape[0], 10)\n",
    "\n",
    "# Reshape the data for 1D CNN input\n",
    "X_train_2HP_1D = X_train_2HP.reshape(X_train_2HP.shape[0], 40, 40)\n",
    "X_train_3HP_1D = X_train_3HP.reshape(X_train_3HP.shape[0], 40, 40)\n",
    "y_train_2HP_1D = y_train_2HP.reshape(y_train_2HP.shape[0], 10)\n",
    "y_train_3HP_1D = y_train_3HP.reshape(y_train_3HP.shape[0], 10)\n",
    "\n",
    "X_test_2HP_1D = X_test_2HP.reshape(X_test_2HP.shape[0], 40, 40)\n",
    "X_test_3HP_1D = X_test_3HP.reshape(X_test_3HP.shape[0], 40, 40)\n",
    "y_test_2HP_1D = y_test_2HP.reshape(y_test_2HP.shape[0], 10)\n",
    "y_test_3HP_1D = y_test_3HP.reshape(y_test_3HP.shape[0], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e1fc33",
   "metadata": {},
   "source": [
    "### Experiment Loop Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca054034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "def save_metrics(metrics, filename, fold, model_type, hp):\n",
    "    \"\"\"\n",
    "    Save model metrics to a JSON file.\n",
    "    \"\"\"\n",
    "    metrics_dir = os.path.join(foldername, \"CNN2D_results_for_report\", \"Metrics\", model_type, hp)\n",
    "    os.makedirs(metrics_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filepath = os.path.join(metrics_dir, f\"{filename}_fold{fold}_{timestamp}.json\")\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "def save_best_model(model, fold, model_type, hp, val_acc):\n",
    "    \"\"\"\n",
    "    Save the best model for a given fold.\n",
    "    \"\"\"\n",
    "    models_dir = os.path.join(foldername, \"CNN2D_results_for_report\", \"Models\", model_type, hp)\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filepath = os.path.join(models_dir, f\"best_model_fold{fold}_val_acc_{val_acc:.4f}_{timestamp}.h5\")\n",
    "    model.save(filepath)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, fold, model_type, hp, class_names):\n",
    "    \"\"\"\n",
    "    Plot and save confusion matrix.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(np.argmax(y_true, axis=1), np.argmax(y_pred, axis=1))\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'Confusion Matrix - {model_type} ({hp}) Fold {fold}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "\n",
    "    plots_dir = os.path.join(foldername, \"CNN2D_results_for_report\", \"Plots\", model_type, hp)\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filepath = os.path.join(plots_dir, f\"confusion_matrix_fold{fold}_{timestamp}.png\")\n",
    "    plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_training_curves(history, fold, model_type, hp):\n",
    "    \"\"\"\n",
    "    Plot and save training/validation accuracy and loss curves.\n",
    "    \"\"\"\n",
    "    plots_dir = os.path.join(foldername, \"CNN2D_results_for_report\", \"Plots\", model_type, hp)\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f'Accuracy Curves - {model_type} ({hp}) Fold {fold}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    filepath = os.path.join(plots_dir, f\"accuracy_curve_fold{fold}_{timestamp}.png\")\n",
    "    plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'Loss Curves - {model_type} ({hp}) Fold {fold}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    filepath = os.path.join(plots_dir, f\"loss_curve_fold{fold}_{timestamp}.png\")\n",
    "    plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def run_experiment(X_train, y_train, X_test, y_test, model_class, kfold, model_type, hp, class_names):\n",
    "    \"\"\"\n",
    "    Run k-fold cross-validation experiment and save results.\n",
    "    \"\"\"\n",
    "    fold_metrics = []\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train, np.argmax(y_train, axis=1))):\n",
    "        print(f\"\\nTraining Fold {fold + 1}/{kfold.n_splits}\")\n",
    "        \n",
    "        model = model_class().model\n",
    "        checkpoint = ModelCheckpoint(\n",
    "            f\"temp_best_model_fold{fold}.h5\",\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            mode='max'\n",
    "        )\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_train[train_idx], y_train[train_idx],\n",
    "            validation_data=(X_train[val_idx], y_train[val_idx]),\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            callbacks=[checkpoint, early_stopping],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        best_model = tf.keras.models.load_model(f\"temp_best_model_fold{fold}.h5\") # Load the best model\n",
    "\n",
    "        train_pred = best_model.predict(X_train[train_idx])\n",
    "        val_pred = best_model.predict(X_train[val_idx])\n",
    "        test_pred = best_model.predict(X_test)\n",
    "        \n",
    "        train_acc = tf.keras.metrics.categorical_accuracy(y_train[train_idx], train_pred).numpy().mean()\n",
    "        val_acc = tf.keras.metrics.categorical_accuracy(y_train[val_idx], val_pred).numpy().mean()\n",
    "        test_acc = tf.keras.metrics.categorical_accuracy(y_test, test_pred).numpy().mean()\n",
    "        \n",
    "        report = classification_report(\n",
    "            np.argmax(y_test, axis=1),\n",
    "            np.argmax(test_pred, axis=1),\n",
    "            target_names=class_names,\n",
    "            output_dict=True\n",
    "        )\n",
    "        \n",
    "        metrics = {\n",
    "            'fold': fold + 1,\n",
    "            'train_accuracy': float(train_acc),\n",
    "            'val_accuracy': float(val_acc),\n",
    "            'test_accuracy': float(test_acc),\n",
    "            'classification_report': report\n",
    "        }\n",
    "        save_metrics(metrics, \"metrics\", fold + 1, model_type, hp)\n",
    "        save_best_model(best_model, fold + 1, model_type, hp, val_acc)\n",
    "        plot_confusion_matrix(y_test, test_pred, fold + 1, model_type, hp, class_names)\n",
    "        plot_training_curves(history, fold + 1, model_type, hp)\n",
    "        \n",
    "        fold_metrics.append(metrics)\n",
    "        os.remove(f\"temp_best_model_fold{fold}.h5\")\n",
    "    \n",
    "    avg_metrics = {\n",
    "        'avg_train_accuracy': float(np.mean([m['train_accuracy'] for m in fold_metrics])),\n",
    "        'avg_val_accuracy': float(np.mean([m['val_accuracy'] for m in fold_metrics])),\n",
    "        'avg_test_accuracy': float(np.mean([m['test_accuracy'] for m in fold_metrics]))\n",
    "    }\n",
    "    save_metrics(avg_metrics, \"average_metrics\", 0, model_type, hp)\n",
    "    \n",
    "    return fold_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37963c7",
   "metadata": {},
   "source": [
    "### Run 4 Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa638289",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\n",
    "    'Normal', 'Inner Race 0.007\"', 'Inner Race 0.014\"', 'Inner Race 0.021\"',\n",
    "    'Ball 0.007\"', 'Ball 0.014\"', 'Ball 0.021\"',\n",
    "    'Outer Race 0.007\"', 'Outer Race 0.014\"', 'Outer Race 0.021\"'\n",
    "]\n",
    "\n",
    "# Run experiments\n",
    "print(\"Running 2HP CNN_2D Experiment\")\n",
    "metrics_2hp_cnn2d = run_experiment(\n",
    "    X_train_2HP_2D, y_train_2HP_2D, X_test_2HP_2D, y_test_2HP_2D,\n",
    "    CNN_2D, kfold, \"CNN_2D\", \"2HP\", class_names\n",
    ")\n",
    "\n",
    "print(\"\\nRunning 2HP CNN_1D Experiment\")\n",
    "metrics_2hp_cnn1d = run_experiment(\n",
    "    X_train_2HP_1D, y_train_2HP_1D, X_test_2HP_1D, y_test_2HP_1D,\n",
    "    CNN_1D, kfold, \"CNN_1D\", \"2HP\", class_names\n",
    ")\n",
    "\n",
    "print(\"\\nRunning 3HP CNN_2D Experiment\")\n",
    "metrics_3hp_cnn2d = run_experiment(\n",
    "    X_train_3HP_2D, y_train_3HP_2D, X_test_3HP_2D, y_test_3HP_2D,\n",
    "    CNN_2D, kfold, \"CNN_2D\", \"3HP\", class_names\n",
    ")\n",
    "\n",
    "print(\"\\nRunning 3HP CNN_1D Experiment\")\n",
    "metrics_3hp_cnn1d = run_experiment(\n",
    "    X_train_3HP_1D, y_train_3HP_1D, X_test_3HP_1D, y_test_3HP_1D,\n",
    "    CNN_1D, kfold, \"CNN_1D\", \"3HP\", class_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102d3a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of loading and displaying metrics\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "def load_latest_metrics(metrics_dir, filename_pattern):\n",
    "    \"\"\"Load the latest JSON file matching the pattern in the given directory.\"\"\"\n",
    "    files = glob.glob(os.path.join(metrics_dir, f\"{filename_pattern}*.json\"))\n",
    "    if not files:\n",
    "        print(f\"No files found in {metrics_dir} for pattern {filename_pattern}\")\n",
    "        return None\n",
    "    latest_file = max(files, key=os.path.getctime)  # Get the most recent file\n",
    "    with open(latest_file, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Load average metrics for all experiments\n",
    "experiments = [\n",
    "    (\"CNN_1D\", \"2HP\", \"average_metrics_fold0\"),\n",
    "    (\"CNN_1D\", \"3HP\", \"average_metrics_fold0\"),\n",
    "    (\"CNN_2D\", \"2HP\", \"average_metrics_fold0\"),\n",
    "    (\"CNN_2D\", \"3HP\", \"average_metrics_fold0\")\n",
    "]\n",
    "\n",
    "for model_type, hp, pattern in experiments:\n",
    "    metrics_dir = os.path.join(\"CNN2D_results_for_report\", \"Metrics\", model_type, hp)\n",
    "    data = load_latest_metrics(metrics_dir, pattern)\n",
    "    if data:\n",
    "        df = pd.DataFrame(data, index=[0])\n",
    "        print(f\"\\nAverage Metrics for {model_type} ({hp}):\")\n",
    "        print(df)\n",
    "\n",
    "# Load and display per-class classification report for a specific fold\n",
    "metrics_dir = os.path.join(\"CNN2D_results_for_report\", \"Metrics\", \"CNN_2D\", \"2HP\")\n",
    "data = load_latest_metrics(metrics_dir, \"metrics_fold1\")\n",
    "if data:\n",
    "    print(\"\\nPer-Class Classification Report (CNN_2D, 2HP, Fold 1):\")\n",
    "    print(f\"Fold: {data['fold']}\")\n",
    "    print(f\"Train Accuracy: {data['train_accuracy']:.4f}\")\n",
    "    print(f\"Validation Accuracy: {data['val_accuracy']:.4f}\")\n",
    "    print(f\"Test Accuracy: {data['test_accuracy']:.4f}\")\n",
    "    \n",
    "    report = data[\"classification_report\"]\n",
    "    class_report = {k: v for k, v in report.items() if isinstance(v, dict)}\n",
    "    df_report = pd.DataFrame(class_report).T\n",
    "    print(\"\\nPer-Class Metrics:\")\n",
    "    print(df_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d9e239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table of average metrics\n",
    "summary_data = []\n",
    "for model_type, hp, pattern in experiments:\n",
    "    metrics_dir = os.path.join(\"CNN2D_results_for_report\", \"Metrics\", model_type, hp)\n",
    "    data = load_latest_metrics(metrics_dir, pattern)\n",
    "    if data:\n",
    "        summary_data.append({\n",
    "            'Model': model_type,\n",
    "            'HP': hp,\n",
    "            'Avg Train Accuracy': data['avg_train_accuracy'],\n",
    "            'Avg Val Accuracy': data['avg_val_accuracy'],\n",
    "            'Avg Test Accuracy': data['avg_test_accuracy']\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nSummary of Average Metrics Across Experiments:\")\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87b7f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate per-class metrics across folds for CNN_2D (2HP)\n",
    "metrics_dir = os.path.join(\"CNN2D_results_for_report\", \"Metrics\", \"CNN_2D\", \"2HP\")\n",
    "fold_files = glob.glob(os.path.join(metrics_dir, \"metrics_fold*.json\"))\n",
    "\n",
    "class_metrics = []\n",
    "for fold_file in fold_files:\n",
    "    with open(fold_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        report = data['classification_report']\n",
    "        for cls in class_names:\n",
    "            class_metrics.append({\n",
    "                'Fold': data['fold'],\n",
    "                'Class': cls,\n",
    "                'Precision': report[cls]['precision'],\n",
    "                'Recall': report[cls]['recall'],\n",
    "                'F1-Score': report[cls]['f1-score']\n",
    "            })\n",
    "\n",
    "class_df = pd.DataFrame(class_metrics)\n",
    "avg_class_metrics = class_df.groupby('Class').mean().reset_index()\n",
    "print(\"\\nAverage Per-Class Metrics for CNN_2D (2HP):\")\n",
    "print(avg_class_metrics[['Class', 'Precision', 'Recall', 'F1-Score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88883308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparing average test accuracies\n",
    "summary_data = []\n",
    "for model_type, hp, pattern in experiments:\n",
    "    metrics_dir = os.path.join(\"CNN2D_results_for_report\", \"Metrics\", model_type, hp)\n",
    "    data = load_latest_metrics(metrics_dir, pattern)\n",
    "    if data:\n",
    "        summary_data.append({\n",
    "            'Model': f\"{model_type} ({hp})\",\n",
    "            'Avg Test Accuracy': data['avg_test_accuracy']\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dd3e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_latest_metrics(metrics_dir, filename_pattern):\n",
    "    files = glob.glob(os.path.join(metrics_dir, f\"{filename_pattern}*.json\"))\n",
    "    if not files:\n",
    "        print(f\"No files found in {metrics_dir} for pattern {filename_pattern}\")\n",
    "        return None\n",
    "    latest_file = max(files, key=os.path.getctime)\n",
    "    try:\n",
    "        with open(latest_file, 'r') as file:\n",
    "            return json.load(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {latest_file}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256db8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check the saved metrics for 3HP CNN_1D\n",
    "# import pandas as pd\n",
    "# import json\n",
    "\n",
    "# with open('CNN2D_results_for_report/Metrics/CNN_1D/2HP/average_metrics_fold0_20250707_144506.json', 'r') as file:\n",
    "#     data = json.load(file)\n",
    "\n",
    "# metrics_2hp_cnn1d = pd.DataFrame(data, index=[0])\n",
    "# metrics_2hp_cnn1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505b0087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "\n",
    "# # Path to your JSON file\n",
    "# json_path = \"/Users/user/Desktop/MEng_UWaterloo/7_ECE_699A_PRJ/Code_Final_Version/CNN2D_results_for_report/Metrics/CNN_2D/2HP/metrics_fold1_20250707_141802.json\"\n",
    "\n",
    "# # Load JSON data\n",
    "# with open(json_path, 'r') as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# # Print basic metrics\n",
    "# print(\"Fold:\", data[\"fold\"])\n",
    "# print(\"Train Accuracy:\", data[\"train_accuracy\"])\n",
    "# print(\"Validation Accuracy:\", data[\"val_accuracy\"])\n",
    "# print(\"Test Accuracy:\", data[\"test_accuracy\"])\n",
    "\n",
    "# # Extract and convert classification report to DataFrame (excluding overall/macro/weighted)\n",
    "# report = data[\"classification_report\"]\n",
    "\n",
    "# # Filter out entries that are not class names\n",
    "# class_report = {k: v for k, v in report.items() if isinstance(v, dict)}\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# df_report = pd.DataFrame(class_report).T  # Transpose for readability\n",
    "\n",
    "# # Display the DataFrame\n",
    "# print(\"\\nPer-Class Classification Report:\")\n",
    "# print(df_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1418eef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
